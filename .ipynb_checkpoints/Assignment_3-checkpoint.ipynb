{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 CS 5316 Natural Language Processing\n",
    "For this assignment we will use the following packages\n",
    "<ul>\n",
    "    <li><a href=\"https://radimrehurek.com/gensim/\">Gensim</a>.</li>\n",
    "    <li><a href=\"https://keras.io/\">Keras</a>.</li>\n",
    "    <li><a href=\"https://www.tensorflow.org/\">Tensorflow</a>.</li>\n",
    "</ul>\n",
    "You can install these packages via anaconda navigator or use the conda install / pip install commands e.g<br>\n",
    "<blockquote>pip install gensim<br>\n",
    "pip install tensorflow<br>\n",
    "pip install keras</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "# Get the interactive Tools for Matplotlib\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import ngrams\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "import keras\n",
    "import seaborn as sn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "Word Vectors nowadays are often used as a fundamental component for downstream NLP tasks, e.g. question answering, text generation, language translation, sentiment analysis, etc. The goal of word embedding methods is to derive a low-dimensional continuous vector representation for words so that words that are syntactically or semantically related are close together in that vector space and thus, share a similar representation.\n",
    "\n",
    "In this assingment we are going to explore different word embedddings inorder to build some intuitions about their strengths and weaknesses. Although there are many types of word embeddings they can be broadly classified into two categories:\n",
    "<ul>\n",
    "    <li>Frequency based Embedding</li>\n",
    "    <li>Prediction based Embedding</li>\n",
    "</ul>\n",
    "For frequenct based embedding we will explore embeddings based on <b>word co-occurance</b> counts with <a href=\"https://en.wikipedia.org/wiki/Pointwise_mutual_information\">Point Wise Mutial Information(PPMI)</a> and <a href=\"https://en.wikipedia.org/wiki/Singular_value_decomposition\">Singular Value Decomposition(SVD)</a>.\n",
    "<a href=\"https://www.youtube.com/watch?v=P5mlg91as1c\">SVD video explaination</a><br>\n",
    "For prediction based embeddings we will explore <a href=\"https://en.wikipedia.org/wiki/Pointwise_mutual_information\">Word2Vec</a> based embeddings.\n",
    "\n",
    "For evaluating these embeddings we will work with the following two datasets: \n",
    "<ul>\n",
    "    <li>Twitter dataset created by Sanders Analytics which we explored in the previous assignment<b>(file provided)</b></li>\n",
    "    <li>Movie reviews dataset from the popular website <a href=\"https://www.imdb.com/\">IMDB</a>.\n",
    "        Head over the to <a href=\"https://www.kaggle.com/c/word2vec-nlp-tutorial/data\">kaggle</a> and download the dataset from there. The dataset consists of three files:<br><b>labelledTrainData,unlabelledTrainData,testData</b></li>   \n",
    "</ul>\n",
    "Read the \"Data\" section on kaggle for details on the dataset.\n",
    "\n",
    "Let's get started.......<br>\n",
    "remove this link later [Assignment solution](https://www.youtube.com/watch?v=dQw4w9WgXcQ&t=40s) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency base Embeddings\n",
    "For this part we will use the Sanders Analytics dataset to create embeddings. Since the other dataset is large we might run into memory problems.<br><br><br>\n",
    "Although we can directly use word representation based on word co-occurance matrix directly it is generally not a good idea to do so for the following reasons:\n",
    "<ul>\n",
    "    <li>The word co-occurance matrix scales with vocabulary size, considering memory constraints this would be problematic for large datasets, as in the IMDB data set that has vocabulary size after remove stop words of 225109, which requres rougly around 189 GiB of storage capacity(roughly 203 GB)</li><img src=\"memoryerror.png\">\n",
    "    <li> The word co-occurance matrix will be quite sparse, meaning many entries in the matrix will be zeros. This is problematic due the fact that for many nlp tasks the multyplication operation is used quite frequently, e.g. for word similarity task, cosine similarity is used:<img src=\"cosine-equation.png\"> Here we can see the dot product is computed between two word vectors, multyplication with zeros wastes precious computation power and your time.</li>\n",
    "    <li> High co-occurance counts for stop words and conjunctions offset true representation of words, meaning thier could become a dominant factor when these embeddings are used in computations. These also dont provide a lot of information as thier counts with other words would also be high.</li>\n",
    "</ul>\n",
    "In summary, you want to avoid sparse represenation's just like the corona virus.<br>\n",
    "To mitigate the above problems we will use PPMI and SVD. PPMI is use to control high co-occurance counts and SVD is used to reduce dimensionality.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Since we have already discussed preprocessing trade off's in previous assingments. We expect you to analyse the data and preform the preprocessing that is required.<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \n",
    "    labels=list(pd.read_csv(filename,usecols=[0]).values)\n",
    "    tweets=list(pd.read_csv(filename,usecols=[1]).values)\n",
    "    tweets=[tweet for i in tweets for tweet in i]\n",
    "    labels=[label for i in labels for label in i]\n",
    "    \n",
    "    return  tweets,labels\n",
    "tweets, labels=load_data(\"twitter-sanders-apple3.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    \"\"\"\n",
    "    Return preprocessed data\n",
    "\n",
    "    Args:\n",
    "        data : reviews\n",
    "    \n",
    "    Returns: preprocessed_data\n",
    "    preprocessed_data : preprocessed dataset \n",
    "    \"\"\"\n",
    "    #remove http\n",
    "    for i in range(0,len(data)):\n",
    "        data[i]=re.sub(r'http:\\/\\/t\\.co\\/.*?[A-Z]$','',data[i])\n",
    "        data[i]=re.sub(r'http[A-Za-z0-9]+','',data[i])\n",
    "    #converting to lowercase\n",
    "    for i in range(0,len(data)):\n",
    "        data[i]=data[i].lower()\n",
    "    #remove punc\n",
    "    for i in range(0,len(data)):\n",
    "        data[i]=re.sub(r'[^0-9a-z\\@\\s]',' ',data[i])\n",
    "        data[i]=data[i].replace('  ',' ')\n",
    "    #add token\n",
    "    for i in range(0,len(data)):\n",
    "        data[i]=re.sub(r'\\@',\"\",data[i])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "data=preprocessing(tweets)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test train split\n",
    "Use test train split from sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testTrainSplit(tweets,labels):\n",
    "    \"\"\"\n",
    "    Return test train data\n",
    "\n",
    "    Args:\n",
    "        data_X : reviews\n",
    "        data_Y: labels\n",
    "    Returns: test train split data \n",
    "    \"\"\"\n",
    "    return train_test_split(tweets,labels,train_size=0.8,test_size=0.2,shuffle=True)\n",
    "\n",
    "train_tweets,test_tweets,train_labels,test_labels=testTrainSplit(data,labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the vocabulary, to find te dimensions of co-occurance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVocabulary(data):\n",
    "    \"\"\"\n",
    "    Return dataset vocabulart\n",
    "\n",
    "    Args:\n",
    "        train_X : reviews in train dataset\n",
    "    \n",
    "    Returns: vocabulary\n",
    "    vocabulary: list of unique words in dataset\n",
    "    \"\"\"\n",
    "    #sen tokenize\n",
    "    sen_tweets=[word_tokenize(i) for i in data]\n",
    "    \n",
    "    #flatten\n",
    "    all_tokens=[j for i in sen_tweets for j in i]\n",
    "    #stop words \n",
    "    lst=list(set(stopwords.words('english')))\n",
    "    all_tokens=[i for i in all_tokens if i not in lst]\n",
    "    #vocab\n",
    "    vocab=list(set(all_tokens))\n",
    "    unk=[]\n",
    "    for i in list(vocab):\n",
    "        if all_tokens.count(i)==1:\n",
    "            unk.append(i)\n",
    "    for i in unk:\n",
    "        if i in vocab:\n",
    "            vocab.remove(i)\n",
    "    \n",
    "    for i,sen in enumerate(sen_tweets):\n",
    "        for j,word in enumerate(sen):\n",
    "            if word in lst:\n",
    "                sen.remove(word)\n",
    "        sen_tweets[i]=sen\n",
    "        \n",
    "    for i,sen in enumerate(sen_tweets):\n",
    "        for j,word in enumerate(sen):\n",
    "            if word not in vocab:\n",
    "                sen[j]='UNK'\n",
    "        sen_tweets[i]=sen\n",
    "    vocab.append('UNK')\n",
    "    dic={}\n",
    "    for i,word in enumerate(vocab):\n",
    "        dic[word]=i\n",
    "        \n",
    "    print(len(vocab))\n",
    "    return vocab,dic,sen_tweets\n",
    "\n",
    "vocab,dic,train=getVocabulary(train_tweets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing sentetnces\n",
    "sen_tokens=[word_tokenize(i) for i in test_tweets]\n",
    "lst=list(set(stopwords.words('english')))\n",
    "test=[]\n",
    "tokens=[]\n",
    "for i in sen_tokens:\n",
    "    for j in i:\n",
    "        if j not in lst:\n",
    "            tokens.append(j)\n",
    "    test.append(tokens)\n",
    "    tokens=[]\n",
    "\n",
    "test_vocab=[]\n",
    "for i in sen_tokens:\n",
    "    for j in i:\n",
    "        test_vocab.append(j)\n",
    "\n",
    "test_vocab=list(set(test_vocab))       \n",
    "len(test_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point Wise Mutial Information\n",
    "Pointwise mutual information, or PMI, is the (unweighted) term that occurs inside of the summation of mutual information and measures the correlation between two specific events. Specifically, PMI is defined as<br>\n",
    "$$PMI(a, b) = \\log \\frac{p(a,b)}{p(a)p(b)}$$\n",
    "\n",
    "and measures the (log) ratio of the joint probability of the two events as compared to the joint probability of the two events assuming they were independent. Thus, PMI is high when the two events a and b co-occur with higher probability than would be expected if they were independent.\n",
    "\n",
    "If we suppose that a and b are words, we can measure how likely we see a and b together compared to what we would expect of they were unrelated by computing their PMI under some model for the joint probability $$p(a,b)$$\n",
    "\n",
    "Let D represent a collection of observed word-context pairs (with contexts being other words). We can construct D by considering the full context of a specific word occurrence as the collection of all word occurrences that appear within a fixed-size window of length L before and after it.\n",
    "\n",
    "For a specific word $w_i$ in position i in a large, ordered collection of words $w_1, w_2$, we would have the context as ,$w_{i-1},w_{i+1},\\ldots$, and could thus collect counts (a total of 2L) of each of those words as appearing in the context of word $w_i$. We will refer to $w_i$ as the “target word” and the words appearing in the L-sized window around $w_i$ as “context words”.\n",
    "\n",
    "Consider a sample corpus containing only one sentence:<br>\n",
    "    <center><blockquote>Encumbered forever by desire and ambition</blockquote></center>\n",
    "\n",
    "We can construct D by considering each word position i and extracting the pairs $(w_i, w_{i+k})$ for $−L≤k≤L;k≠0$. In such a pair, we would call $w_i$ the “target word” and $w_{i+k}$ the “context word”.\n",
    "\n",
    "For example, we would extract the following pairs for $i=4i$ if we let our window size $L=2$<br>\n",
    "    <center><blockquote>(desire,forever),(desire,by),(desire,and),(desire,ambition)</blockquote></center>\n",
    "\n",
    "Similarly, for $i=5i$, we would extract the following pairs:\n",
    "    <center><blockquote>(and,by),(and,desire),(and,ambition)</blockquote></center>\n",
    "Let’s let $n_{w,c}$ represent the number of times we observe word type c in the context of word type w. We can then define ,$n_w = \\sum_{c'} n_{w,c'}$ as the number of times we see a “target” word w in the collection of pairs D and $n_c = \\sum_{w'} n_{w',c}$ as the number of times we see the context word c in the collection of pairs D.\n",
    "\n",
    "We can then define the joint probability of a word and a context word as\n",
    "    $$p(w, c) = \\frac{n_{w,c}}{|D|}$$\n",
    "\n",
    "where $∣D∣$ is simply the total number of word-context occurrences we see. Similarly, we can define\n",
    "    $$p(w) = \\frac{n_w}{|D|}$$\n",
    "\n",
    "and $$p(c) = \\frac{n_c}{|D|}$$\n",
    "\n",
    "and thus the PMI between a word w and context word c is\n",
    "$$PMI(w, c) = \\log \\frac{p(w,c)}{p(w)p(c)} = \\log \\frac{n_{w,c} \\cdot |D|}{n_w \\cdot n_c}.$$\n",
    "\n",
    "If we compute the PMI between all pairs of words in our vocabulary V, we will arrive at a large, real-valued matrix. However, some of the values of this matrix will be $\\log 0$, if the word-context pair $(w,c)$ is unobserved, this will result in inf bieng computed. To remedy this, we could simply define a modified PMI that is equal to 0 when $n_{w,c} = 0$, which is the positive pointwise mutual information (PPMI) which:\n",
    "    P$$PPMI(w,c) = \\max(0, PMI(w,c))$$\n",
    "<br>\n",
    "This wonderfull explaination is made by <a href=\"http://czhai.cs.illinois.edu/\">Dr.ChengXiang (\"Cheng\") Zhai</a><br><br>\n",
    "\n",
    "<center><b>HINT: Consult your slides and see the example, how the formulas are used. You can calculate $|D|$ by the formula given in the slides(its the same thing).</b></center>\n",
    "\n",
    "If youre having troubles implementing this here is some [motivation](https://www.youtube.com/watch?v=TsyM5jP7RQk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a co-occurance matrix with +,- k window size\n",
    "Hint: Use the ngrams package from [nltk](https://www.nltk.org/) to make life easier. Matrix size is vocab X vocab.\n",
    "Please keep track of the order of words in the matrix this will be usefull later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "def coOccuranceMatrix(sen_tokens,vocab,dic,k=2):\n",
    "    \"\"\"\n",
    "    Return co-occurance matrix with ppmi counts\n",
    "\n",
    "    Args:\n",
    "        data : dataset\n",
    "        vocab : vocabulary\n",
    "    Returns: co_matrix\n",
    "    co_matrix: co-occurance matrix\n",
    "    \"\"\"\n",
    "    matrix = np.zeros((len(vocab),len(vocab)))\n",
    "\n",
    "  \n",
    "    # <- <- -kwindow\n",
    "    for sen in sen_tokens:\n",
    "        ngs=list(ngrams(sen,k+1))\n",
    "        for tup in ngs:\n",
    "            target=tup[0]\n",
    "            for i in range(1,len(tup)):\n",
    "                context=tup[i]\n",
    "                matrix[dic[target]][dic[context]]+=1\n",
    "    # -> -> +kwindow\n",
    "    for sen in sen_tokens:\n",
    "        ngs=list(ngrams(sen,k+1))\n",
    "        for tup in ngs:\n",
    "            target=tup[-1]\n",
    "            for i in range(0,len(tup)-1):\n",
    "                context=tup[i]\n",
    "                matrix[dic[target]][dic[context]]+=1\n",
    "                \n",
    "    \n",
    "    \n",
    "    \n",
    "    return matrix\n",
    "\n",
    "matrix=coOccuranceMatrix(train,vocab,dic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ppmiMatrix(co_matrix):\n",
    "    \"\"\"\n",
    "    Return co-occurance matrix with ppmi counts\n",
    "\n",
    "    Args:\n",
    "        co_matrix : co-occurance matrix\n",
    "    Returns: ppmi_matrix\n",
    "    ppmi_co_matrix: co-occurance matrix with ppmi counts\n",
    "    \"\"\"\n",
    "    col_totals = co_matrix.sum(axis=0)\n",
    "    total = col_totals.sum()\n",
    "    row_totals = co_matrix.sum(axis=1)\n",
    "    expected = np.outer(row_totals, col_totals) / total\n",
    "    co_matrix = co_matrix / expected\n",
    "    \n",
    "    #taken from stackoverflow\n",
    "    with np.errstate(divide='ignore'):\n",
    "        co_matrix = np.log(co_matrix)\n",
    "    co_matrix[np.isinf(co_matrix)] = 0\n",
    "    co_matrix[co_matrix < 0] = 0\n",
    "    np.nan_to_num(co_matrix, copy=False)\n",
    "    \n",
    "    return co_matrix\n",
    "    \n",
    "\n",
    "ppmatrix=ppmiMatrix(matrix)\n",
    "ppmatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for SVD has been provided for you, all you have to do is specify the number of top eigenvalues or how many top <b>n</b> dimensions you want to keep. Check the dimensions of the returned matrix by using <blockquote>.shape</blockquote> command to figure out if the embedding for each word is in row or column. By our calculation the vocab count should be less than five thousand, reduce the dimensionality to less than one thousand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_size=len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code provided \n",
    "def denseMatrixViaSVD(ppmi_co_matrix,n):\n",
    "    \"\"\"\n",
    "    Return reduced dimensionality co-occurance matrix by applying svd\n",
    "\n",
    "    Args:\n",
    "        ppmi_matrix : co-occurance matrix with ppmi counts\n",
    "        \n",
    "    Returns: svd_co_matrix\n",
    "    svd_co_matrix: reduced dimensionality co-occurance matrix\n",
    "    \"\"\"\n",
    "#     top_n_eigenvalues=\n",
    "    U, Sigma, VT = randomized_svd(ppmi_co_matrix, \n",
    "                              n_components=n,\n",
    "                              n_iter=5,\n",
    "                              random_state=None)\n",
    "    svd_co_matrix=U\n",
    "    return svd_co_matrix\n",
    "dense=denseMatrixViaSVD(ppmatrix,e_size)\n",
    "dense \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "Now that we have our embeddings, lets use these to train a Feed Forward Neural network for our semantic classification task. Since a feed forward network's input layer is of a fixed size we will need to create a fixed size representation for each review. For this purpose we will use the following:\n",
    "<ul>\n",
    "    <li>Average pooling.</li>\n",
    "    <li>Averaging pooling algorithm by FastText(provided)</li>\n",
    "    <li>Max pooling. </li>\n",
    "</ul>\n",
    "For those of you who are familiar with Convolution Neural Networks this pooling will be a 1d pooling operation. See illustrated example below:<img src=\"pooling.png\">\n",
    "\n",
    "Since we cant have a tutorial due to corona virus for keras, a simple feed forward network has beed provided for you. You need to create train_X, test_X , train_Y and test_Y these should numpy arrays inorder for keras to use them.\n",
    "<ul>\n",
    "    <li>train_X= contains embedding representains of all the reviews in the train set</li>\n",
    "    <li>train_Y= contains embedding representains of all the reviews in the test set</li>\n",
    "    <li>train_Y= contains <b>one hot</b> representations of train labels</li>\n",
    "    <li>test_Y= contains <b>one hot</b> representations of test labels</li>   \n",
    "</ul>\n",
    "To construct one hot representation you can use the sklearn's preprocessing package or the preprocessing package from keras. Read online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTRUCT ONE HOT REPRESENTATION\n",
    "#[neg0,neu1,pos2]\n",
    "def make_labels(train_labels):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from keras.utils import to_categorical\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(train_labels)\n",
    "    return to_categorical(integer_encoded)\n",
    "labels=make_labels(train_labels)\n",
    "print(labels[1],train_labels[1])\n",
    "labels1=make_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fast text averaging, pass a list of word embeddings and embedding size to fasttextAveraging function\n",
    "def l2_norm(x):\n",
    "   return np.sqrt(np.sum(x**2))\n",
    "\n",
    "def div_norm(x):\n",
    "   norm_value = l2_norm(x)\n",
    "   if norm_value > 0:\n",
    "       return x * ( 1.0 / norm_value)\n",
    "   else:\n",
    "       return x\n",
    "def fasttextAveraging(embedding_list,embedding_size):\n",
    "    norm=np.zeros(embedding_size)\n",
    "    for emb in embedding_list:\n",
    "        norm=norm+div_norm(emb) \n",
    "    return norm/len(embedding_list)\n",
    "\n",
    "tweets=[]\n",
    "tweets1=[]\n",
    "tw=[]\n",
    "for j in train:\n",
    "    for i in j:\n",
    "        tw.append(dense[dic[i]])\n",
    "    ans=fasttextAveraging(tw,e_size)\n",
    "    tweets.append(ans)\n",
    "    tw=[]\n",
    "tweets=np.array(tweets)\n",
    "\n",
    "matrix1=np.array((len(test_vocab),len(test_vocab)))\n",
    "\n",
    "for i in test:\n",
    "    for j in i:\n",
    "        if j not in vocab:\n",
    "            tw.append(dense[dic['UNK']])\n",
    "        else:\n",
    "            tw.append(dense[dic[j]])\n",
    "    ans=fasttextAveraging(tw,e_size)\n",
    "    tweets1.append(ans)\n",
    "    tw=[]\n",
    "tweets1=np.array(tweets1)\n",
    "tweets.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def averagePooling(embedding_list,embedding_size):\n",
    "    \"\"\"\n",
    "    Return average embedding vector from list of embedding\n",
    "    Args:\n",
    "        embedding_list : embedding list\n",
    "        embedding_size: size of embedding vector\n",
    "    Returns: average_embedding\n",
    "    average_embedding: average embedding vector\n",
    "    \"\"\"\n",
    "        \n",
    "    return np.mean(embedding_list,axis=0)\n",
    "\n",
    "avg_tweets=[]\n",
    "avg_tweets1=[]\n",
    "tw=[]\n",
    "for j in train:\n",
    "    for i in j:\n",
    "        tw.append(dense[dic[i]])\n",
    "    ans=averagePooling(tw,e_size)\n",
    "    avg_tweets.append(ans)\n",
    "    tw=[]\n",
    "avg_tweets=np.array(avg_tweets)\n",
    "\n",
    "\n",
    "for i in test:\n",
    "    for j in i:\n",
    "        if j not in vocab:\n",
    "            tw.append(dense[dic['UNK']])\n",
    "        else:\n",
    "            tw.append(dense[dic[j]])\n",
    "    ans=averagePooling(tw,e_size)\n",
    "    avg_tweets1.append(ans)\n",
    "    tw=[]\n",
    "avg_tweets1=np.array(avg_tweets1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxPooling(embedding_list,embedding_size):\n",
    "    \"\"\"\n",
    "    Return maxpooling embedding vector from list of embedding\n",
    "    Args:\n",
    "        embedding_list : embedding list\n",
    "        embedding_size: size of embedding vector\n",
    "    Returns: max_embedding\n",
    "    max_embedding: maxpooled embedding vector\n",
    "    \"\"\"\n",
    "    arr= np.zeros(embedding_size)\n",
    "    for i in embedding_list:\n",
    "        arr+=np.max(i,axis=0)\n",
    "        \n",
    "    return arr\n",
    "\n",
    "max_tweets=[]\n",
    "max_tweets1=[]\n",
    "tw=[]\n",
    "for j in train:\n",
    "    for i in j:\n",
    "        tw.append(dense[dic[i]])\n",
    "    ans=maxPooling(tw,e_size)\n",
    "    max_tweets.append(ans)\n",
    "    tw=[]\n",
    "max_tweets=np.array(max_tweets)\n",
    "print(max_tweets[0])\n",
    "\n",
    "for i in test:\n",
    "    for j in i:\n",
    "        if j not in vocab:\n",
    "            tw.append(dense[dic['UNK']])\n",
    "        else:\n",
    "            tw.append(dense[dic[j]])\n",
    "    ans=maxPooling(tw,e_size)\n",
    "    max_tweets1.append(ans)\n",
    "    tw=[]\n",
    "max_tweets1=np.array(max_tweets1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using all three representaions to train the model and check which one works best. You can play around with embedding size by controlling <b>n</b> in SVD function and for the model you can add or remove layers or change the number of neurons in the hidden layers. Keep in mind that the layers should be decreasing in size as we go deeper into the network, theoritically this means that we are constructing complex features in a lower dimensional space from less complex features and larger dimensional space.<br><br>\n",
    "Issues related to overfiting will be proper addressed in the next assignment for now you are free to choose the number of epoch, try to find one that trains the model sufficiently enough but does not overfit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
    "filepath = \"setting_\" + \"model1\" + \".hdf5\"\n",
    "logfilepath = \"setting_\"+\"model1\" + \".csv\"\n",
    "reduce_lr_rate=0.2\n",
    "logCallback = CSVLogger(logfilepath, separator=',', append=False)\n",
    "earlyStopping = EarlyStopping(monitor='acc', min_delta=0, patience=10, verbose=0, mode='max')\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='acc', save_weights_only=True, verbose=1,\n",
    "                             save_best_only=True, mode='max')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='acc', factor=reduce_lr_rate, patience=10,\n",
    "                              cooldown=0, min_lr=0.0000000001, verbose=0)\n",
    "\n",
    "callbacks_list = [logCallback, earlyStopping, reduce_lr, checkpoint]\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "embedding_size=e_size\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(embedding_size,)),#donot change(input layer)\n",
    "    keras.layers.Dense(500, activation='relu'),#(hidden layer)\n",
    "    keras.layers.Dense(100, activation='relu'),#(hidden layer)\n",
    "    keras.layers.Dense(3)#donot change\n",
    "])\n",
    "adam=keras.optimizers.Adam(lr=0.00001)\n",
    "model.compile(optimizer=adam,\n",
    "              loss=[\"categorical_crossentropy\"],\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(avg_tweets,labels, epochs=15, batch_size=32,\n",
    "               verbose=1,shuffle=True,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelList=['neg','neu','pos']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the <b>model.predict</b> method to get predictions. There predictions will be a probability distribution over the lables, to get the desired class take the max value in a prediction vector as the predicted class.<br> To run the code below you need to construct a list of unique labels, the list should be ordered on the basis of the id assigned to each class when you were constructing the one hot representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(max_tweets1)\n",
    "pred=[]\n",
    "for i in predictions:\n",
    "    pred.append(np.argmax(i))\n",
    "predictions=pred\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "test_Y_max=np.argmax(labels1, axis=-1)\n",
    "cm=confusion_matrix(test_Y_max,predictions)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "cm = pd.DataFrame(cm, labelList,labelList )# matrix,names row,names col,\n",
    "# plt.figure(figsize=(10,7))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 11}, fmt=\".2f\") # font size\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(\"Classification Report\\n\",sklearn.metrics.classification_report(test_Y_max, predictions, labels=[0,1,2], target_names = labelList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction base Embeddings\n",
    "For prediction based embeddings we will use the IMDB dataset. We will create create our embeddings by using the unlabeledTrainData.tsv file.\n",
    "We will use the Word2Vec model that we have already covered in class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Since we have already discussed preprocessing trade off's in previous assingments. We expect you to analyse the data and preform the preprocessing that is required.<br> \n",
    "<b> Hint: Each review is in string format so they have used slahes to escape characters and br tags to identify line breaks</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "import pandas as pd \n",
    "unlabeled=pd.read_csv('unlabeled.txt',sep=\"\\t\",error_bad_lines=False,encoding='utf-8')\n",
    "unlabeled=list(unlabeled['review'])\n",
    "labeled=pd.read_csv('labeledTrainData.tsv',sep='\\t',encoding='utf-8')\n",
    "data=list(labeled['review'])\n",
    "labels=list(labeled['sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocessing(data):\n",
    "    \"\"\"\n",
    "    Return preprocessed data\n",
    "\n",
    "    Args:\n",
    "        data : reviews\n",
    "    \n",
    "    Returns: preprocessed_data\n",
    "    preprocessed_data : preprocessed dataset \n",
    "    \"\"\"\n",
    "    for i,text in enumerate(data):\n",
    "        data[i]=text.lower()\n",
    "    \n",
    "    \n",
    "    for i,text in enumerate(data):\n",
    "        data[i]=re.findall(r'[A-Za-z]+',text)\n",
    "    \n",
    "    lst=list(set(stopwords.words('english')))\n",
    "    tokens=[]\n",
    "    all_tokens=[]\n",
    "    for num,i in enumerate(data):\n",
    "        for j in i:\n",
    "            if len(j)<=2:\n",
    "                continue\n",
    "            if j in lst:\n",
    "                continue\n",
    "            else:\n",
    "                tokens.append(j)\n",
    "        all_tokens.append(tokens)\n",
    "        tokens=[]\n",
    "        \n",
    "    data=all_tokens\n",
    "        \n",
    "    \n",
    "    \n",
    "    return data\n",
    "data= preprocessing(data)\n",
    "unlabeled=preprocessing(unlabeled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews,test_reviews,train_labels,test_labels=testTrainSplit(data,labels)\n",
    "len(train_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_voc(data):\n",
    "    vocab=[]\n",
    "    for i in data:\n",
    "        for j in i:\n",
    "            vocab.append(j)\n",
    "    vocab=list(set(vocab))\n",
    "    return vocab\n",
    "unlabeled_voc=get_voc(unlabeled)\n",
    "\n",
    "vocab_dic={}\n",
    "for i,v in enumerate(unlabeled_voc):\n",
    "    vocab_dic[v]=i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [gensim](https://radimrehurek.com/gensim/models/word2vec.html) to train a Word2Vec model. Keep the dimensionality at 300 and window size at 2. After trianing use the model and previously coded methods create vectorial represenations for movie reviews.<i>(create train_X, test_X, train_Y and test_Y)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "def trainWord2Vec(data):\n",
    "    \"\"\"\n",
    "    Return preprocessed data\n",
    "\n",
    "    Args:\n",
    "        data : movie reviews\n",
    "    \n",
    "    Returns: model\n",
    "    model : Word2Vec model \n",
    "    \"\"\"\n",
    "    path = get_tmpfile(\"word2vec.model\")\n",
    "    model = Word2Vec(data, size=300, window=2, min_count=1, workers=4)\n",
    "    model.save(\"word2vec.model\")\n",
    "    return model\n",
    "model=trainWord2Vec(unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the train and test files and create the vectorial representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw=[]\n",
    "train=[]\n",
    "for i in train_reviews:\n",
    "    for j in i:\n",
    "        try:\n",
    "            word=vocab_dic[j]\n",
    "        except:\n",
    "            continue\n",
    "        tw.append(model.wv[j])\n",
    "    ans=fasttextAveraging(tw,300)\n",
    "    train.append(ans)\n",
    "    tw=[]\n",
    "\n",
    "train=np.array(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw=[]\n",
    "test=[]\n",
    "for i in test_reviews:\n",
    "    for j in i:\n",
    "        try:\n",
    "            word=vocab_dic[j]\n",
    "        except:\n",
    "            continue\n",
    "        tw.append(model.wv[j])\n",
    "    ans=maxPooling(tw,300)\n",
    "    test.append(ans)\n",
    "    tw=[]\n",
    "\n",
    "test=np.array(test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=make_labels(train_labels)\n",
    "labels1=make_labels(test_labels)\n",
    "labels.shape\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is dense representaion we wont be faced with the challenges posed by sparse representations. We can move onto modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
    "filepath = \"setting_\" + \"model1\" + \".hdf5\"\n",
    "logfilepath = \"setting_\"+\"model1\" + \".csv\"\n",
    "reduce_lr_rate=0.2\n",
    "logCallback = CSVLogger(logfilepath, separator=',', append=False)\n",
    "earlyStopping = EarlyStopping(monitor='loss', min_delta=0, patience=3, verbose=0, mode='min')\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', save_weights_only=True, verbose=1,\n",
    "                             save_best_only=True, mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=reduce_lr_rate, patience=10,\n",
    "                              cooldown=0, min_lr=0.0000000001, verbose=0)\n",
    "\n",
    "callbacks_list = [logCallback, earlyStopping, reduce_lr, checkpoint]\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "embedding_size=300\n",
    "model_word2vec = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(embedding_size,)),#donot change(input layer)\n",
    "    keras.layers.Dense(300, activation='relu'),#(hidden layer)\n",
    "    keras.layers.Dense(50, activation='relu'),#(hidden layer)\n",
    "    keras.layers.Dense(2)#donot change\n",
    "])\n",
    "adam=keras.optimizers.Adam(lr=0.00001)\n",
    "model_word2vec.compile(optimizer=adam,\n",
    "              loss=[\"categorical_crossentropy\"],\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_word2vec.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec.fit(train,labels, epochs=15, batch_size=32,\n",
    "               verbose=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_word2vec.predict(test)\n",
    "pred=[]\n",
    "for i in predictions:\n",
    "    pred.append(np.argmax(i))\n",
    "predictions=np.array(pred)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "labelList=['<5','>7']\n",
    "test_Y_max=np.argmax(labels1, axis=-1)\n",
    "cm=confusion_matrix(test_Y_max,predictions)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "cm = pd.DataFrame(cm, labelList,labelList )# matrix,names row,names col,\n",
    "# plt.figure(figsize=(10,7))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 11}, fmt=\".2f\") # font size\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report\\n\",sklearn.metrics.classification_report(test_Y_max, predictions, labels=[0,1], target_names = labelList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory\n",
    "The two are two major reaserch papers [Word2Vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) for prediction based embeddings and [GloVe](https://nlp.stanford.edu/pubs/glove.pdf) for frequency based embeddings. Research online and write a short note on the trade offs associated with the two types of embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###_______________Anwer________________###\n",
    "\n",
    "The word2vec performs better than GloVe model.Word2vec offers best vector representation in lower representation.GloVe model performance can be made similar to the word2vec by increasing the semantic space. GloVe performs better when there is wide variety of grammatical forms and complex morphology is involved in the dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ending Note:\n",
    "Feed forward networks are not suitable for natural language task because of thier fixed input sizes, the size of natural language text in each example for a dataset can vary considerably, also feed forward networks ignore the temporal nature of natural language text, which result's in them not bieng able to caputre context's or interdepencies between words for semantic information. To fix this researcher's have invented recurrent neural networks that help to aleviate these limitations.\n",
    "The next assignment will be related to recurrent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We hope all of you are working on your projects!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
